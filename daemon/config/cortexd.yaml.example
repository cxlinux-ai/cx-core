# Cortexd Daemon Configuration
# Copy this file to /etc/cortex/daemon.yaml or ~/.cortex/daemon.yaml

# Socket configuration
socket:
  path: /run/cortex/cortex.sock
  backlog: 16
  timeout_ms: 5000

# LLM configuration
llm:
  # Backend type: "local", "cloud_claude", "cloud_openai", or "none"
  # - local: Use local llama.cpp server (cortex-llm.service)
  # - cloud_claude: Use Anthropic Claude API
  # - cloud_openai: Use OpenAI API
  # - none: Disable LLM features (default)
  backend: "none"

  # Cloud API configuration (when backend: cloud_claude or cloud_openai)
  cloud:
    # API key environment variable name (key is read from env, not stored here)
    # Default for cloud_claude: ANTHROPIC_API_KEY
    # Default for cloud_openai: OPENAI_API_KEY
    api_key_env: ""

  # Local llama.cpp configuration (when backend: local)
  local:
    # URL of the cortex-llm service (llama.cpp server)
    base_url: "http://127.0.0.1:8085"

  # Legacy embedded LLM settings (deprecated - use cortex-llm.service instead)
  # These settings are kept for backwards compatibility but will be removed
  # Path to GGUF model file (leave empty to disable embedded LLM)
  model_path: ""
  # Context length (tokens)
  context_length: 2048
  # Number of CPU threads for inference
  threads: 4
  # Batch size for prompt processing
  batch_size: 512
  # Load model on first request instead of startup
  lazy_load: true
  # Use memory mapping for model (recommended)
  mmap: true

# System monitoring configuration
monitoring:
  # Check interval in seconds
  interval_sec: 300
  # Enable APT package monitoring
  enable_apt: true
  # Enable CVE vulnerability scanning
  enable_cve: true
  # Enable dependency conflict checking
  enable_deps: true

# Alert thresholds (0.0 - 1.0)
thresholds:
  # Disk usage warning threshold (80%)
  disk_warn: 0.80
  # Disk usage critical threshold (95%)
  disk_crit: 0.95
  # Memory usage warning threshold (85%)
  mem_warn: 0.85
  # Memory usage critical threshold (95%)
  mem_crit: 0.95

# Alert configuration
alerts:
  # SQLite database path for alert persistence
  db_path: ~/.cortex/alerts.db
  # Alert retention period in hours (7 days)
  retention_hours: 168
  # Enable AI-powered alert analysis (requires LLM model loaded)
  # When enabled, alerts include intelligent suggestions from the LLM
  enable_ai: true

# Rate limiting
rate_limit:
  # Maximum IPC requests per second
  max_requests_per_sec: 100
  # Maximum inference queue size
  max_inference_queue: 100

# Logging level (0=DEBUG, 1=INFO, 2=WARN, 3=ERROR)
log_level: 1

